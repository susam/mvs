Mark V. Shaney
==============

This project is a minimal implementation of Mark V. Shaney in Python.
Mark V. Shaney was a synthetic Usenet user that posted various
messages to the newsgroups using text generated by a Markov chain
program.  See the Wikipedia article [Mark V. Shaney][mvs-wiki] for
more details about it.

The program [mvs.py][] available in this project consumes the content
of the file [book.txt](book.txt), builds an internal model and then
uses the model to generate gibberish.

**[View Source][mvs.py]**

[book.txt]: book.txt
[mvs.py]: mvs.py
[mvs-wiki]: https://en.wikipedia.org/wiki/Mark_V._Shaney


Contents
--------

* [Source Code](#source-code)
* [Get Started](#get-started)
* [Command Line Arguments](#command-line-arguments)
* [Output Examples](#output-examples)
  * [Without Prompts](#without-prompts)
  * [With Prompts](#with-prompts)
* [Licence](#licence)


Source Code
-----------

Here is the complete source code of the gibberish generator
([mvs.py][]):

```python3
#!/usr/bin/env python3

import random
import sys

type Key = tuple[str, ...]
type Model = dict[Key, list[str]]


def train(text: str, n: int) -> Model:
    words = text.split()
    model: Model = {}
    for i in range(len(words) - n):
        key = tuple(words[i : i + n])
        value = words[i + n]
        model.setdefault(key, []).append(value)
    return model


def generate(model: Model, length: int, prompt: Key) -> str:
    key = prompt if prompt else random.choice(list(model.keys()))
    output = list(key)
    for _ in range(length - len(key)):
        values = model.get(key)
        if not values:
            break
        next_word = random.choice(values)
        output.append(next_word)
        key = *key[1:], next_word
    return " ".join(output)


def main(n: int, length: int, prompt: Key) -> None:
    model = train(open("book.txt").read(), n)
    print(generate(model, length, prompt[:n]))


if __name__ == "__main__":
    main(
        int(sys.argv[1]) if len(sys.argv) > 1 else 2,
        int(sys.argv[2]) if len(sys.argv) > 2 else 100,
        tuple(sys.argv[3].split()) if len(sys.argv) > 3 else (),
    )
```

This program implements a simple Markov text generator.  It reads
[book.txt][] and records every sequence of *n* consecutive words
together with the words that follow them.  From this data it learns
which words tend to come next after a given word sequence, with more
frequent sequences being more likely to be chosen while generating
text.

To generate text, the program starts from a random sequence or a user
provided prompt.  It repeatedly selects a possible following word at
random, weighted by how often it appeared in the original text.  The
result mimics the local patterns of the source material while drifting
into grammatically plausible but often meaningless gibberish.

This implementation is deliberately simple and inefficient.  It keeps
all observed word sequences and their followers in memory, including
duplicates, which makes the model needlessly large.  There is plenty
of scope for improvement, such as storing follower frequencies instead
of keeping duplicate entries or pruning rarely seen sequences.

More sophisticated techniques can be applied to produce more plausible
sounding text, including higher order n-grams, careful handling of
sentence boundaries and punctuation, addition of grammatical
constraints or probabilistic language models.  What is provided here
is intended to serve as a minimum viable Markov text generator.  Any
further enhancements are left as an exercise for the reader.

Given the overwhelming popularity of large language models (LLMs) in
2025, it is worth noting that this approach bears little resemblance
to LLMs.  LLMs are trained on vast datasets using neural networks to
model language patterns across large spans of text.  LLMs capture
global structure and long range dependencies.  By contrast, Markov
text generators rely entirely on local word transition statistics and
have no model of global structure.  Despite these limitations, the
Markov text generator shared in this project can serve as a simple and
introduction to statistical language modelling.  After all, Markov
chains can be thought of as the 'hello, world' of language models.


Get Started
-----------

To get started with the [mvs.py][] program available in this project,
clone or download this project to your system and then enter the
following command:

```sh
python3 mvs.py
```

This will generate arbitrary gibberish based on the model it has built
by consuming the text in the file [book.txt](book.txt).  By default,
the content of this file is the entire text of the book *A Christmas
Carol* by Charles Dickens.  You can replace the content of this file
by any other piece of text to generate gibberish based on that.


Command Line Arguments
----------------------

To keep this tool as minimal as possible, it does not come with any
command line options.  In fact, it does not even have the `--help`
option.  However, it supports a few command line arguments.  Since
there is no help output from the tool, this section describes the
command line arguments for this tool.

Here is a synopsis of the command line arguments supported by this tool:

```
python3 mvs.py [N [LENGTH [PROMPT]]]
```

Here is a description of each argument:

  - `N`

    The order of the Markov model.  This value specifies how many
    consecutive words are used as the state when training the model.
    For example, a value of 2 builds a bigram model, while a value of
    3 builds a trigram model.  If not specified, this defaults to 2.

  - `LENGTH`

    The maximum number of words to generate.  Generation may stop
    earlier if the model reaches a state for which no continuation
    exists.  If not specified, this defaults to 100.


  - `PROMPT`

    An optional starting prompt used to seed text generation.  This
    should be a single command line argument containing one or more
    words separated by spaces, so it must be quoted when invoking the
    program.  If provided, only the first N words of the prompt are
    used.  If omitted, generation starts from a random state in the
    model.

Here are some usage examples of these command line arguments:

  1. Generate gibberish using a trigram model:

     ```sh
     python3 mvs.py 3
     ```

  2. Generate gibberish up to 250 words long:

     ```sh
     python3 mvs.py 2 250
     ```

  3. Use the words 'There is' to start the gibberish:

     ```sh
     python3 mvs.py 2 100 'There is'
     ```


Output Examples
---------------


### Without Prompts

```
$ python3 mvs.py
can't help thinking that a night of unbroken rest would have been an
affront to your father at the door, and there was no escape; then
his conduct was the same breath, laughing as he looked the phantom
through and through, and saw it standing before him; and his
coat-skirts, and the two young Cratchits went to fetch them, and
especially to observe what happened next. They were a boy singing a
Christmas carol: but at the idea of knocking Scrooge down with it,
for Scrooge to approach, which he felt the chilling influence of its
own hinges, I believe;
```

```
$ python3 mvs.py
round its head, as before. Scrooge knew the men, and they were in
another scene and place; a room, not very large or handsome, but
full of promise, might have endeavoured to diffuse in vain. For, the
people saw him in both his arms, and forced him to observe the
shadow of its own act. But there was a Turkey! He never could have
done it all at once, tripped lightly off to sea in butter-boats,
hundreds of the shops, that here too it was next to impossible to
keep it all in a broken voice, "remove me from this
```

```
$ python3 mvs.py
streamed upon it when the Bell struck One, and no one seemed to
shine. This idea taking full possession of his bed were drawn aside;
and Scrooge, starting off again, and thought, and carried him
along. They scarcely seemed to spring up about them, and especially
to observe what happened next. They were men of business: very
wealthy, and of my mind to feast upon, and I say, he began to wonder
which of his head, and questioned beggars, and looked up at the
office next morning. Oh, he was an eager, greedy, restless motion in
the back-shop. During the
```

### With Prompts


```
$ python3 mvs.py 2 100 'I think'
I think he loses some pleasant moments, which could hardly stand when
he came peeping round the bed. For he had set his heart leap up as
they went. Every one of them! Though I never eat lunch. But I'll offer
to go, if anybody could have done it too--at last, I say, God bless
us!" Which all the world. It must be allowed to have let loose waves
of hair, an inch of which would bring in, if obtained, full
five-and-sixpence weekly. The two young Cratchits hustled Tiny Tim,
and felt with us." "I'm sure he's a good old
```

```
$ python3 mvs.py 2 100 'There is'
There is no doubt about that. The register of his chamber. He was so
inexpressibly tickled, that he stood there, waiting his arrival, the
knocker on the floor, and back came Tiny Tim drank it last of the
blaze showed preparations for a moment you were free to-day,
to-morrow, yesterday, can even I believe that it seemed to shine. This
idea taking full possession of his monstrous shirt collar (Bob's
private property, conferred upon his knees, and clasped him gently by
the clergyman, the clerk, who, cold as he had set his heart leap up as
they went along, Scrooge
```

```
$ python3 mvs.py 2 100 'At last'
At last she said, amazed, "there is! Nothing is past hope, if such a
man whose name he had an expectation that the singer fled in terror,
leaving the keyhole to regale him with such favour, that he turned his
steps towards his door. "It's humbug still!" said Scrooge. "I am very
happy," said little Bob, the father, who came upon his knee; for in
the fire, and deep red curtains, ready to our calling, we're well
matched. Come into the most extravagant contortions: Scrooge's niece,
indignantly. Bless those women; they never do anything by halves. They
are all indescribable
```


Licence
-------

This is free and open source software.  You can use, copy, modify,
merge, publish, distribute, sublicence and/or sell copies of it, under
the terms of the MIT Licence.  See [LICENSE.md][L] for details.

This software is provided "AS IS", WITHOUT WARRANTY OF ANY KIND,
express or implied.  See [LICENSE.md][L] for details.

[L]: LICENSE.md
